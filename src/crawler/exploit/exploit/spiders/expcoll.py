from scrapy.contrib.spiders import Rule,CrawlSpider
from scrapy.selector import Selector
from scrapy.http import Request
from exploit.items import ExploitItem
import scrapy,string,urllib

class ExploitSpider(CrawlSpider):
	name = "expcoll"
	allowed_domains = ["exploit-db.com"]
	start_urls = ["http://www.exploit-db.com/platform/?p=linux/"]

	def parse(self, response):
		sel = response.xpath("//tr")
		for s in sel:
			item = ExploitItem()
			item["date"] = s.xpath(".//td[@class='list_explot_date']/text()").extract()
			item["dlink"] = s.xpath(".//td[@class='list_explot_dlink']/a/@href").extract()
			item["desc"] = s.xpath(".//td[@class='list_explot_description']/a/text()").extract()
			item["author"] = s.xpath(".//td[@class='list_explot_author']/a/@title").extract()
			print item["dlink"]

			if item["dlink"]:
				x = ''.join(item["dlink"])
				filename = x.split('/')[-2]
				print x
				urllib.urlretrieve(x,filename+'.c')

		text = response.xpath("//a[@class='color']/text()").extract()
		print text[1]
		if text[1] == 'next':
			url = response.xpath("//a[@class='color']/@href").extract()
			print url[1]
			if url[1]:
				yield scrapy.Request(url[1],callback=self.parse)
