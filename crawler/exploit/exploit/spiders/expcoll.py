from scrapy.contrib.spiders import Rule,CrawlSpider
from scrapy.selector import Selector
from scrapy.http import Request
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
import scrapy

from exploit.items import ExploitItem

class ExploitSpider(CrawlSpider):
	name = "expcoll"
	allowed_domains = ["exploit-db.com"]
	start_urls = ["http://www.exploit-db.com/platform/?p=linux/"]

	def parse(self, response):
		sel = response.xpath("//table/tbody/tr")
		rows = []
		for s in sel:
			item = ExploitItem()
			item["date"] = s.xpath("//td[@class='list_explot_date']/text()").extract()
			item["dlink"] = s.xpath("//td[@class='list_explot_dlink']/a/@href").extract()
			item["desc"] = s.xpath("//td[@class='list_explot_description']/a/text()").extract()
			item["author"] = s.xpath("//td[@class='list_explot_author']/a/@title").extract()
			rows.append(item)
			#extract exploit file present in download link
		yield rows

		text = response.xpath("//a[@class='color']/text()").extract()
		print text[1]
		if text[1] == 'next':
			url = response.xpath("//a[@class='color']/@href").extract()
			print url
			if url[1]:
				yield scrapy.Request(url[1],callback=self.parse)

		#return rows
